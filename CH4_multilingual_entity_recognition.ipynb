{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 코랩을 사용하지 않으면 이 셀의 코드를 주석 처리하세요.\n!git clone https://github.com/rickiepark/nlp-with-transformers.git\n%cd nlp-with-transformers\nfrom install import *\ninstall_requirements(chapter=4)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:48:05.314861Z","iopub.execute_input":"2023-01-05T07:48:05.315220Z","iopub.status.idle":"2023-01-05T07:48:38.503836Z","shell.execute_reply.started":"2023-01-05T07:48:05.315187Z","shell.execute_reply":"2023-01-05T07:48:38.502484Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Cloning into 'nlp-with-transformers'...\nremote: Enumerating objects: 532, done.\u001b[K\nremote: Counting objects: 100% (266/266), done.\u001b[K\nremote: Compressing objects: 100% (159/159), done.\u001b[K\nremote: Total 532 (delta 161), reused 184 (delta 107), pack-reused 266\u001b[K\nReceiving objects: 100% (532/532), 45.97 MiB | 11.64 MiB/s, done.\nResolving deltas: 100% (260/260), done.\n/kaggle/working/nlp-with-transformers\n⏳ Installing base requirements ...\n✅ Base requirements installed!\nUsing transformers v4.20.1\nUsing datasets v2.1.0\nUsing accelerate v0.12.0\nUsing sentencepiece v0.1.97\nUsing seqeval\nNo GPU was detected! This notebook can be *very* slow without a GPU 🐢\nGo to Settings > Accelerator and select GPU.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 다중 언어 개체명 인식\n- 다중 언어 트랜스포머는 많은 언어로 된 대규모 말뭉치에서 사전 훈련해서 제로샷 교차 언어 전이가 가능하다\n- 한 언어에서 미세 튜닝된 모델이 추가 훈련 없이 다른 언어에 적용 된다는 의미이다.\n- 또 이런 모델은 한 대화에서 둘 이상의 언어나 사투리를 바꾸는 코드 스위칭에 적합하다.\n- 이 장에서는 XLM-RpBERTa 트랜스포머 모델을 개체명 인식을 수행하도록 여러 언어에서 미세 투닝하는 방법을 알아보겠다.\n- NER은 텍스트에서 사람, 조직, 위치 같은 개체명을 식별하는 일반적은 NLP작업으로 다양한 애플리케이션에서 사용한다.\n- 가량 회사 문서에서 중요한 정보를 추출하거나, 검색 엔진의 품질을 높이거나, 말뭉치에서 구조적인 데이터베이스를 만든다.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntoks = \"Jeff Dean is a computer scientist at Google in California\".split()\nlbls = [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-ORG\", \"O\", \"B-LOC\"]\ndf = pd.DataFrame(data=[toks, lbls], index=['Tokens', 'Tags'])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:48:38.506071Z","iopub.execute_input":"2023-01-05T07:48:38.506421Z","iopub.status.idle":"2023-01-05T07:48:38.527860Z","shell.execute_reply.started":"2023-01-05T07:48:38.506371Z","shell.execute_reply":"2023-01-05T07:48:38.526484Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            0      1   2  3         4          5   6       7   8           9\nTokens   Jeff   Dean  is  a  computer  scientist  at  Google  in  California\nTags    B-PER  I-PER   O  O         O          O   O   B-ORG   O       B-LOC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>Jeff</td>\n      <td>Dean</td>\n      <td>is</td>\n      <td>a</td>\n      <td>computer</td>\n      <td>scientist</td>\n      <td>at</td>\n      <td>Google</td>\n      <td>in</td>\n      <td>California</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>B-PER</td>\n      <td>I-PER</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-ORG</td>\n      <td>O</td>\n      <td>B-LOC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names('xtreme')\nprint(f'XTREME 서브셋 개수 : {len(xtreme_subsets)}')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:48:38.529350Z","iopub.execute_input":"2023-01-05T07:48:38.529706Z","iopub.status.idle":"2023-01-05T07:48:41.389679Z","shell.execute_reply.started":"2023-01-05T07:48:38.529675Z","shell.execute_reply":"2023-01-05T07:48:41.388857Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b44737f72b54afca1ec85f960b43259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/23.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad561163d1a4632bfa6fe49c8c061f0"}},"metadata":{}},{"name":"stdout","text":"XTREME 서브셋 개수 : 183\n","output_type":"stream"}]},{"cell_type":"code","source":"panx_subsets = [s for s in xtreme_subsets if s.startswith('PAN')]\npanx_subsets[:3]","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:48:41.391667Z","iopub.execute_input":"2023-01-05T07:48:41.392581Z","iopub.status.idle":"2023-01-05T07:48:41.400938Z","shell.execute_reply.started":"2023-01-05T07:48:41.392547Z","shell.execute_reply":"2023-01-05T07:48:41.399889Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['PAN-X.af', 'PAN-X.ar', 'PAN-X.bg']"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nload_dataset('xtreme', name='PAN-X.de')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:48:41.402135Z","iopub.execute_input":"2023-01-05T07:48:41.402585Z","iopub.status.idle":"2023-01-05T07:49:13.855823Z","shell.execute_reply.started":"2023-01-05T07:48:41.402552Z","shell.execute_reply":"2023-01-05T07:49:13.854670Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.de (download: 223.17 MiB, generated: 9.08 MiB, post-processed: Unknown size, total: 232.25 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dabad4ece3a418bbcc9e5b48307ced0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.de/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b88bed4579c40d99e8b5235430a00a6"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 20000\n    })\n    validation: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'langs'],\n        num_rows: 10000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from collections import defaultdict\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# 키가 없는 경우 DatasetDict를 반환합니다.\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # 다국어 말뭉치를 로드합니다.\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # 각 분할을 언어 비율에 따라 다운샘플링하고 섞습니다.\n    for split in ds:\n        panx_ch[lang][split] = (\n            ds[split]\n            .shuffle(seed=0)\n            .select(range(int(frac * ds[split].num_rows))))","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:13.857317Z","iopub.execute_input":"2023-01-05T07:49:13.857705Z","iopub.status.idle":"2023-01-05T07:49:47.739014Z","shell.execute_reply.started":"2023-01-05T07:49:13.857672Z","shell.execute_reply":"2023-01-05T07:49:47.737956Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"585fb6e3921a4568b1ebfa35a7698764"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.fr (download: 223.17 MiB, generated: 6.37 MiB, post-processed: Unknown size, total: 229.53 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.fr/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58eab739e0754ceca987b4b2c288a82c"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.it (download: 223.17 MiB, generated: 7.35 MiB, post-processed: Unknown size, total: 230.52 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.it/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6efc370cebf446c7b5295810af3f4d36"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xtreme/PAN-X.en (download: 223.17 MiB, generated: 7.30 MiB, post-processed: Unknown size, total: 230.47 MiB) to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xtreme downloaded and prepared to /root/.cache/huggingface/datasets/xtreme/PAN-X.en/1.0.0/349258adc25bb45e47de193222f95e68a44f7a7ab53c4283b3f007208a11bf7e. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80730675d8554acb8eaa76e42b9197ab"}},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame({lang : [panx_ch[lang]['train'].num_rows] for lang in langs},\n             index=['Number of training examples'])","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:47.740286Z","iopub.execute_input":"2023-01-05T07:49:47.740623Z","iopub.status.idle":"2023-01-05T07:49:47.756847Z","shell.execute_reply.started":"2023-01-05T07:49:47.740594Z","shell.execute_reply":"2023-01-05T07:49:47.755646Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                de    fr    it    en\nNumber of training examples  12580  4580  1680  1180","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>de</th>\n      <th>fr</th>\n      <th>it</th>\n      <th>en</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of training examples</th>\n      <td>12580</td>\n      <td>4580</td>\n      <td>1680</td>\n      <td>1180</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"element = panx_ch['de']['train'][0]\nfor key, value in element.items():\n  print(f'{key} : {value}')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:47.758243Z","iopub.execute_input":"2023-01-05T07:49:47.758628Z","iopub.status.idle":"2023-01-05T07:49:47.772966Z","shell.execute_reply.started":"2023-01-05T07:49:47.758587Z","shell.execute_reply":"2023-01-05T07:49:47.771770Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"tokens : ['2.000', 'Einwohnern', 'an', 'der', 'Danziger', 'Bucht', 'in', 'der',\n'polnischen', 'Woiwodschaft', 'Pommern', '.']\nner_tags : [0, 0, 0, 0, 5, 6, 0, 0, 5, 5, 6, 0]\nlangs : ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de']\n","output_type":"stream"}]},{"cell_type":"code","source":"for key, value in panx_ch['de']['train'].features.items():\n  print(f'{key}:{value}')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:47.774080Z","iopub.execute_input":"2023-01-05T07:49:47.774495Z","iopub.status.idle":"2023-01-05T07:49:47.783433Z","shell.execute_reply.started":"2023-01-05T07:49:47.774455Z","shell.execute_reply":"2023-01-05T07:49:47.782279Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tokens:Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\nner_tags:Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER',\n'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)\nlangs:Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"tags = panx_ch['de']['train'].features['ner_tags'].feature\nprint(tags)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:47.787860Z","iopub.execute_input":"2023-01-05T07:49:47.788367Z","iopub.status.idle":"2023-01-05T07:49:47.794644Z","shell.execute_reply.started":"2023-01-05T07:49:47.788330Z","shell.execute_reply":"2023-01-05T07:49:47.793267Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n'B-LOC', 'I-LOC'], id=None)\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_tag_names(batch):\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:47.796033Z","iopub.execute_input":"2023-01-05T07:49:47.796509Z","iopub.status.idle":"2023-01-05T07:49:52.346507Z","shell.execute_reply.started":"2023-01-05T07:49:47.796473Z","shell.execute_reply":"2023-01-05T07:49:52.345494Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12580 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62356fe399742ab859f702a6f5d5ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d6599d9ac1404d8ab5dc80d0dc43c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6290 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc8087d49dfa422a8cc9a39dadb1fb47"}},"metadata":{}}]},{"cell_type":"code","source":"de_example = panx_de['train'][0]\npd.DataFrame([de_example['tokens'], de_example['ner_tags_str']], ['Tokens', 'Tags'])","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:52.348165Z","iopub.execute_input":"2023-01-05T07:49:52.348624Z","iopub.status.idle":"2023-01-05T07:49:52.369318Z","shell.execute_reply.started":"2023-01-05T07:49:52.348578Z","shell.execute_reply":"2023-01-05T07:49:52.368126Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"           0           1   2    3         4      5   6    7           8   \\\nTokens  2.000  Einwohnern  an  der  Danziger  Bucht  in  der  polnischen   \nTags        O           O   O    O     B-LOC  I-LOC   O    O       B-LOC   \n\n                  9        10 11  \nTokens  Woiwodschaft  Pommern  .  \nTags           B-LOC    I-LOC  O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Tokens</th>\n      <td>2.000</td>\n      <td>Einwohnern</td>\n      <td>an</td>\n      <td>der</td>\n      <td>Danziger</td>\n      <td>Bucht</td>\n      <td>in</td>\n      <td>der</td>\n      <td>polnischen</td>\n      <td>Woiwodschaft</td>\n      <td>Pommern</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>Tags</th>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n      <td>O</td>\n      <td>B-LOC</td>\n      <td>B-LOC</td>\n      <td>I-LOC</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from collections import Counter\n\nsplit2freqs = defaultdict(Counter)\nfor split, dataset in panx_de.items():\n  for row in dataset['ner_tags_str']:\n    for tag in row:\n      if tag.startswith('B'):\n        tag_type = tag.split('-')[1]\n        split2freqs[split][tag_type] += 1\n\npd.DataFrame.from_dict(split2freqs, orient='index')","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:52.371925Z","iopub.execute_input":"2023-01-05T07:49:52.372339Z","iopub.status.idle":"2023-01-05T07:49:52.746326Z","shell.execute_reply.started":"2023-01-05T07:49:52.372307Z","shell.execute_reply":"2023-01-05T07:49:52.745255Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"             LOC   ORG   PER\ntrain       6186  5366  5810\nvalidation  3172  2683  2893\ntest        3180  2573  3071","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LOC</th>\n      <th>ORG</th>\n      <th>PER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>6186</td>\n      <td>5366</td>\n      <td>5810</td>\n    </tr>\n    <tr>\n      <th>validation</th>\n      <td>3172</td>\n      <td>2683</td>\n      <td>2893</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>3180</td>\n      <td>2573</td>\n      <td>3071</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## 2. 다중 언어 트랜스포머\n- 다중 언어 트랜스포머의 훈련 과정과 아키텍처는 단일 언어 트랜스포머와 비슷하다.\n- 다만 사전훈련에 사용하는 말뭉치가 여러 언어의 문서로 구성될 뿐이다.\n- 언어의 차이에 대한 정보가 명시적으로 제공되지 않아도, 이렇게 구축한 언어 표현이 여러 언어의 다양한 후속 작업에 쉽게 일반화된다는 점이 놀라운 특징이다.\n- 교차 언어 전이를 수행하는 이런 능력이 경우에 따라 단일 언어 모델과 비슷한 결과를 내므로 언어마다 별도의 모델 훈련을 할 필요가 없다.","metadata":{}},{"cell_type":"markdown","source":"## 3.XLM-R 토큰화\n- XLM-R은 WordPiece 토크나이저 대신 100개 언어의 텍스트에서 훈련도니 SentencePiece라는 토크나이저를 사용한다.\n- SentencePiece를 WordPiece와 비교하기 위해 트랜스포머스로 BERT와 XLM-R의 토크나이저를 로드하겠다.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbert_model_name = 'bert-base-cased'\nxlmr_model_name = 'xlm-roberta-base'\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:49:52.747650Z","iopub.execute_input":"2023-01-05T07:49:52.747955Z","iopub.status.idle":"2023-01-05T07:50:22.885890Z","shell.execute_reply.started":"2023-01-05T07:49:52.747928Z","shell.execute_reply":"2023-01-05T07:50:22.884590Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6bdc1bcd45b47cab28fc9ace6513391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80df332c93f8479984696882d497cdf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73dee446aa5407891f387f31da0b4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9591b68afc7c4f248fe0eca1c927bdf1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b800746928bb42ba99eead6b69d4bffa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc97b9774d8465bba267a2039c0b9f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0aa62e08c6d4c53a26d0aa120f7890c"}},"metadata":{}}]},{"cell_type":"code","source":"text = 'Jack Sparrow loves New York!'\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:50:22.887622Z","iopub.execute_input":"2023-01-05T07:50:22.887996Z","iopub.status.idle":"2023-01-05T07:50:22.897015Z","shell.execute_reply.started":"2023-01-05T07:50:22.887964Z","shell.execute_reply":"2023-01-05T07:50:22.896021Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame([bert_tokens, xlmr_tokens], index=[\"BERT\", \"XLM-R\"])\ndf","metadata":{"execution":{"iopub.status.busy":"2023-01-05T07:50:22.898159Z","iopub.execute_input":"2023-01-05T07:50:22.899418Z","iopub.status.idle":"2023-01-05T07:50:22.915582Z","shell.execute_reply.started":"2023-01-05T07:50:22.899360Z","shell.execute_reply":"2023-01-05T07:50:22.914695Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"           0      1      2       3      4    5     6      7      8     9\nBERT   [CLS]   Jack    Spa  ##rrow  loves  New  York      !  [SEP]  None\nXLM-R    <s>  ▁Jack  ▁Spar     row  ▁love    s  ▁New  ▁York      !  </s>","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>BERT</th>\n      <td>[CLS]</td>\n      <td>Jack</td>\n      <td>Spa</td>\n      <td>##rrow</td>\n      <td>loves</td>\n      <td>New</td>\n      <td>York</td>\n      <td>!</td>\n      <td>[SEP]</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>XLM-R</th>\n      <td>&lt;s&gt;</td>\n      <td>▁Jack</td>\n      <td>▁Spar</td>\n      <td>row</td>\n      <td>▁love</td>\n      <td>s</td>\n      <td>▁New</td>\n      <td>▁York</td>\n      <td>!</td>\n      <td>&lt;/s&gt;</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### 1) 토큰화 파이프라인\n- 지금까지 토큰화를 다룰 때 문자열을 모델에 주입할 정수로 변환하는 연산으로만 여겼는데 전적으로 옳은것은 아니다.\n- 실제로는 정규화, 사전 토큰화, 토크나이저 모델, 사후 처리 이 네 단게로 구성된다.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}